import os
import re
import requests
import json
import aiohttp
import asyncio
import unicodedata
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
import random  # åŠ å…¥é¡¶éƒ¨
import openpyxl

# === éšæœº User-Agent åˆ—è¡¨ï¼ˆé˜²åçˆ¬ï¼‰ ===
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/115.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",
]


MAX_PAGES_LIMIT = 50  # æœ€å¤šé‡‡é›†æ¯ä¸ªå…³é”®è¯çš„50é¡µï¼Œé˜²æ­¢æ­»å¾ªç¯


# === ç¯å¢ƒå˜é‡ä¸é…ç½®è¯»å– ===
load_dotenv()
BILIBILI_COOKIE = os.getenv("BILIBILI_COOKIE")

# === æ–‡ä»¶è·¯å¾„é…ç½® ===
INPUT_CSV = "C:/Code/python/animeproject/repair.xlsx"
OUTPUT_CSV = "C:/Code/python/animeproject/fanmade_results_movie_repair.csv"


# === äºŒåˆ›ç›¸å…³å…³é”®è¯å®šä¹‰ ===
FANMADE_SUFFIXES = [" MAD", " äºŒåˆ›", " æ··å‰ª", " åŒäºº", " AMV", " é¬¼ç•œ"]
FANMADE_TAGS = ["äºŒåˆ›", "MAD", "æ··å‰ª", "åŒäºº", "é¬¼ç•œ", "AMV", "å‰ªè¾‘"]

# === é—²è§’å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤æ— æ„ä¹‰è§’è‰²ï¼‰ ===
EXCLUDE_ROLE_KEYWORDS = [
    "ãƒ¢ãƒ–", "ãƒ¢ãƒ–ã‚­ãƒ£ãƒ©", "ãƒ¢ãƒ–ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼", "ã‚¨ã‚­ã‚¹ãƒˆãƒ©",
    "è·¯äºº", "ç¾¤ä¼—", "ç¾¤æ¼”", "å°å…µ", "å­¦ç”Ÿ", "è€å¸ˆ", "è®°è€…", "åŠ©æ‰‹",
    "é€šè¡Œäºº", "è§‚ä¼—", "æ‘äºº", "æ‘æ°‘", "ç”·æ€§", "å¥³æ€§", "å°‘å¹´", "å°‘å¥³",
    "äººç±»", "åº—å‘˜", "ä¿å®‰", "å¸æœº", "åŒ»ç”Ÿ", "å£«å…µ", "ä¹˜å®¢", "æœåŠ¡å‘˜",
    "extra", "mob character", "é—²è§’"
]

# === å·¥å…·å‡½æ•° ===
def normalize_keyword(text):
    """æ ‡å‡†åŒ–å…³é”®è¯ï¼ˆå…¨è§’è½¬åŠè§’å¹¶å»é™¤ç©ºæ ¼ï¼‰"""
    return unicodedata.normalize('NFKC', text).strip() if isinstance(text, str) else ""

def is_valid_character_name(name):
    """åˆ¤æ–­åå­—æ˜¯å¦æœ‰æ•ˆä¸”ä¸å±äºé—²è§’"""
    return bool(name) and not any(x in name for x in EXCLUDE_ROLE_KEYWORDS)

def contains_any(text, keywords):
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯"""
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in keywords)

def read_anime_names(csv_path):
    """ä»CSVæ–‡ä»¶ä¸­è¯»å–åŠ¨æ¼«åç§°åˆ—è¡¨"""
    df = pd.read_excel(csv_path, sheet_name="movie")  # è¯»å–æŒ‡å®š sheet
    anime_list = df["movie"].dropna().tolist()  # æå–éç©ºçš„â€œmovieâ€åˆ—ä¸ºåˆ—è¡¨
    print(f"âœ… æˆåŠŸè¯»å– {len(anime_list)} ä¸ªåŠ¨æ¼«åç§°")
    return anime_list

def generate_search_keywords(extended_keywords):
    """è¿”å›æ‰©å±•å…³é”®è¯åˆ—è¡¨"""
    return extended_keywords



async def search_all_pages(keyword):
    all_results = []
    async with aiohttp.ClientSession() as session:
        page = 1
                # å…ˆè®¿é—®ä¸€æ¬¡ HTML é¡µé¢æ¨¡æ‹Ÿæ­£å¸¸æµè§ˆå™¨è¡Œä¸º
        search_page_url = f"https://search.bilibili.com/all?keyword={keyword}"
        await session.get(search_page_url, headers={"User-Agent": random.choice(USER_AGENTS)})
        await asyncio.sleep(random.uniform(1.0, 2.0))

        while page <= MAX_PAGES_LIMIT:
            results = await safe_bvid_search(session, keyword, page)
            if not results:
                break
            print(f"âœ… å…³é”®è¯ã€{keyword}ã€‘ç¬¬ {page} é¡µï¼Œè·å– {len(results)} æ¡")
            all_results.extend(results)
            page += 1
            await asyncio.sleep(random.uniform(0.8, 2.0))  # åŠ ä¸€ç‚¹å»¶è¿Ÿæ›´ç¨³å¦¥
    return all_results





# === Bangumi APIï¼šè·å–è¡ç”Ÿå…³é”®è¯ ===
def get_extended_keywords(anime_name):
    print(f"\nğŸ” æ­£åœ¨å¤„ç†ï¼š{anime_name}")
    keywords = {anime_name}
    headers = {"User-Agent": "Mozilla/5.0", "Content-Type": "application/json"}
    valid_keys = {"åˆ«å", "åˆ«ç§°", "åˆç§°", "è‹±æ–‡", "ç®€ç§°", "ç¼©å†™", "ç®€å†™", "cp", "æƒ…ä¾£", "è§’è‰²", "ç™»åœºè§’è‰²", "ä¸»è¦è§’è‰²"}

    try:
        # === 1. è·å– subject_id ===
        search_url = "https://api.bgm.tv/v0/search/subjects"
        res = requests.post(search_url, json={"keyword": anime_name, "filter": {"type": [2]}, "sort": "rank", "limit": 3}, headers=headers, timeout=10)
        subject_list = res.json().get("data", [])
        if not subject_list:
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ¡ç›®")
            return list(keywords)

        subject = subject_list[0]
        subject_id = subject.get("id")
        keywords.update(filter(None, [subject.get("name", "").strip(), subject.get("name_cn", "").strip()]))
        keywords.update(subject.get("other_names", []))

        # === 2. è·å– infobox ä¿¡æ¯ ===
        detail_url = f"https://api.bgm.tv/v0/subjects/{subject_id}"
        infobox = requests.get(detail_url, headers=headers, timeout=10).json().get("infobox", [])

        for entry in infobox:
            key = entry.get("key", "").lower()
            val = entry.get("value", "")
            if any(k.lower() in key for k in valid_keys):
                items = [v.get("v", "") if isinstance(v, dict) else v for v in val] if isinstance(val, list) else re.split(r"[ã€ï¼Œ,\s/]", val)
                keywords.update(filter(None, map(str.strip, items)))

        # === 3. è·å–è§’è‰²åŠåˆ«å ===
        char_url = f"https://api.bgm.tv/v0/subjects/{subject_id}/characters"
        characters = requests.get(char_url, headers=headers, timeout=10).json()
        print(f"âœ… å…±æ‰¾åˆ° {len(characters)} ä¸ªè§’è‰²")

        for char in characters:
            for name in [char.get("name", ""), char.get("name_cn", "")]:
                name = name.strip()
                if is_valid_character_name(name):
                    keywords.add(name)

            # è·å–è§’è‰²åˆ«å
            detail_data = requests.get(f"https://api.bgm.tv/v0/characters/{char['id']}", headers=headers, timeout=10).json()
            for entry in detail_data.get("infobox", []):
                if any(k in entry.get("key", "").lower() for k in ["åˆ«å", "ç®€ç§°", "cp", "ç§°å‘¼", "åˆå«", "å¤–å·"]):
                    val = entry.get("value", "")
                    items = [v.get("v", "") if isinstance(v, dict) else v for v in val] if isinstance(val, list) else re.split(r"[ã€ï¼Œ,\s/]", val)
                    keywords.update(filter(lambda x: is_valid_character_name(x.strip()), map(str.strip, items)))

    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼š{e}")

    # === æ ‡å‡†åŒ–ã€å»é‡ã€è¿‡æ»¤ ===
    final_keywords = {normalize_keyword(k) for k in keywords if len(normalize_keyword(k)) >= 2 and is_valid_character_name(normalize_keyword(k))}
    final_list = sorted(final_keywords)
    print(f"ğŸ“Œ æœ€ç»ˆå…³é”®è¯ï¼ˆ{len(final_list)} é¡¹ï¼‰ï¼š{final_list}")
    return final_list



# === æ§åˆ¶å¹¶å‘æ•°é‡ï¼ˆé˜²æ­¢ IP è¢«å°ï¼‰ ===
sem = asyncio.Semaphore(8)

async def safe_bvid_search(session, keyword, page):
    async with sem:
        return await bvid_search(session, keyword, page)


# === å¼‚æ­¥ç‰ˆæœ¬ B ç«™æœç´¢æ¥å£ ===
async def bvid_search(session, keyword, page, retries=3):
    url = "https://api.bilibili.com/x/web-interface/search/type"
    params = {"search_type": "video", "keyword": keyword, "page": page}
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://www.bilibili.com/",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Connection": "keep-alive",
        "Origin": "https://www.bilibili.com",
        "Accept": "application/json",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Cookie": BILIBILI_COOKIE
    }
    for attempt in range(retries):
        try:
            async with session.get(url, params=params, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                json_data = await resp.json()
            return json_data.get("data", {}).get("result", [])
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼š{keyword} - ç¬¬{page}é¡µ - é”™è¯¯ï¼š{e}")
            return []


# === äºŒåˆ›è§†é¢‘ç­›é€‰é€»è¾‘ ===
def is_official_author(name):
    return any(k in name for k in ["å®˜æ–¹", "å‡ºå“", "åˆ¶ä½œå§”å‘˜ä¼š", "åŠ¨ç”»å®˜æ–¹"])

def contains_official_tag(tags):
    return contains_any(tags, ["PV", "å…ˆå¯¼", "é¢„å‘Š", "å®£ä¼ ", "ç‰‡æ®µ", "èŠ±çµ®", "å®˜æ–¹"])

async def collect_fanmade_videos(anime_list):
    results = []
    for anime_name in tqdm(anime_list, desc="å¤„ç†åŠ¨æ¼«ä¸­"):
        extended_keywords = get_extended_keywords(anime_name)
        search_keywords = generate_search_keywords(extended_keywords)
        main_names = [anime_name] + [kw for kw in extended_keywords if anime_name in kw and kw != anime_name]

        for keyword in search_keywords:
            await asyncio.sleep(random.uniform(3.0, 5.0))  # å…³é”®è¯é—´å»¶è¿Ÿé˜²åçˆ¬
            all_results = await search_all_pages(keyword)
            for video in all_results:
                title = video.get("title", "")
                desc = video.get("description", "")
                tags = video.get("tag", "")
                author = video.get("author", "")

                if (
                    contains_any(title, extended_keywords)
                    and contains_any(tags, FANMADE_TAGS)
                    and contains_any(tags, main_names)
                    and not contains_official_tag(tags)
                    and not is_official_author(author)
                ):
                    results.append({
                        "æ‰€å±åŠ¨æ¼«": anime_name,
                        "è§†é¢‘æ ‡é¢˜": title,
                        "æ’­æ”¾é‡": video.get("play"),
                        "å¼¹å¹•æ•°": video.get("video_review"),
                        "æ”¶è—æ•°": video.get("favorites"),
                        "UPä¸»": author,
                        "æ ‡ç­¾": tags,
                        "bvid": video.get("bvid"),
                        "url": f"https://www.bilibili.com/video/{video.get('bvid')}"
                    })
    return results

# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    anime_names = read_anime_names(INPUT_CSV)
    video_data = asyncio.run(collect_fanmade_videos(anime_names))  # âœ… æ­£ç¡®è¿è¡Œåç¨‹
    pd.DataFrame(video_data).to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"\nâœ… å…±ä¿å­˜ {len(video_data)} æ¡è§†é¢‘è®°å½•åˆ° {OUTPUT_CSV}")

