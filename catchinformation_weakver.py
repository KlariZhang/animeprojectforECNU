import requests  # ç”¨äºå‘é€HTTPè¯·æ±‚
import pandas as pd  # ç”¨äºæ•°æ®å¤„ç†å’ŒCSVæ“ä½œ
import time  # ç”¨äºæ·»åŠ å»¶è¿Ÿ
from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡
import re  # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œ
from dotenv import load_dotenv
import os
import json  # æ”¾åœ¨æ–‡ä»¶å¼€å¤´ç”¨äºæ‰“å°æ ¼å¼åŒ–çš„JSON
from bs4 import BeautifulSoup
import unicodedata

load_dotenv()  # è¯»å–.envæ–‡ä»¶
BILIBILI_COOKIE = os.getenv("BILIBILI_COOKIE")  # è·å– Cookie


# === é…ç½®å‚æ•° ===
INPUT_CSV = "C:/Code/python/animeproject/anime_list.csv"  # è¾“å…¥CSVæ–‡ä»¶åï¼ŒåŒ…å«åŠ¨æ¼«åˆ—è¡¨
OUTPUT_CSV = "C:/Code/python/animeproject/fanmade_results.csv"  # è¾“å‡ºCSVæ–‡ä»¶åï¼Œä¿å­˜äºŒåˆ›è§†é¢‘ç»“æœ
MAX_PAGES_PER_KEYWORD = 3  # æ¯ä¸ªå…³é”®è¯æœ€å¤šæœç´¢çš„é¡µæ•°

# äºŒåˆ›è§†é¢‘å¸¸è§çš„åç¼€è¯ï¼ˆç”¨äºç”Ÿæˆæœç´¢å…³é”®è¯ï¼‰
FANMADE_SUFFIXES = [" MAD", " äºŒåˆ›", " æ··å‰ª", " åŒäºº", " AMV", " é¬¼ç•œ"]
# äºŒåˆ›è§†é¢‘å¸¸è§çš„æ ‡ç­¾ï¼ˆç”¨äºè¯†åˆ«è§†é¢‘ç±»å‹ï¼‰
FANMADE_TAGS = ["äºŒåˆ›", "MAD", "æ··å‰ª", "åŒäºº", "é¬¼ç•œ", "AMV", "å‰ªè¾‘"]


# === æ­¥éª¤ 1ï¼šè¯»å–è¾“å…¥åŠ¨æ¼«å ===
def read_anime_names(csv_path):
    """ä»CSVæ–‡ä»¶ä¸­è¯»å–åŠ¨æ¼«åç§°åˆ—è¡¨"""
    df = pd.read_csv(csv_path)  # è¯»å–CSVæ–‡ä»¶
    print(f"âœ… æˆåŠŸè¯»å– {len(df["anime"].dropna().tolist())} ä¸ªåŠ¨æ¼«åç§°")  # âœ… added for debug
    return df["anime"].dropna().tolist()  # è¿”å›"anime"åˆ—çš„éç©ºå€¼åˆ—è¡¨


# === æ­¥éª¤ 2ï¼šè‡ªåŠ¨æ‰©å±•å…³é”®è¯ï¼š===


# âœ… æ ‡å‡†åŒ–å…³é”®è¯ï¼ˆç»Ÿä¸€å…¨è§’/åŠè§’ï¼Œå»ç©ºæ ¼ï¼‰
def normalize_keyword(text):
    if not text or not isinstance(text, str):
        return ""
    return unicodedata.normalize('NFKC', text).strip()

# âœ… é—²è§’å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤æ‰æ— æ„ä¹‰è§’è‰²ï¼‰
EXCLUDE_ROLE_KEYWORDS = [
    "ãƒ¢ãƒ–", "ãƒ¢ãƒ–ã‚­ãƒ£ãƒ©", "ãƒ¢ãƒ–ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼", "ã‚¨ã‚­ã‚¹ãƒˆãƒ©",
    "è·¯äºº", "ç¾¤ä¼—", "ç¾¤æ¼”", "å°å…µ", "å­¦ç”Ÿ", "è€å¸ˆ", "è®°è€…", "åŠ©æ‰‹",
    "é€šè¡Œäºº", "è§‚ä¼—", "æ‘äºº", "æ‘æ°‘", "ç”·æ€§", "å¥³æ€§", "å°‘å¹´", "å°‘å¥³",
    "äººç±»", "åº—å‘˜", "ä¿å®‰", "å¸æœº", "åŒ»ç”Ÿ", "å£«å…µ", "ä¹˜å®¢", "æœåŠ¡å‘˜",'extra', 'mob character','é—²è§’'
]

def is_valid_character_name(name):
    return name and not any(x in name for x in EXCLUDE_ROLE_KEYWORDS)

def get_extended_keywords(anime_name):
    print(f"\nğŸ” æ­£åœ¨å¤„ç†ï¼š{anime_name}")
    keywords = set()
    keywords.add(anime_name)

    # âœ… æœ‰æ•ˆå­—æ®µå…³é”®è¯ï¼ˆç”¨äºæ¨¡ç³ŠåŒ¹é… infobox çš„ keyï¼‰
    valid_keys = set([
        "åˆ«å", "åˆ«ç§°", "åˆç§°", "è‹±æ–‡", "ç®€ç§°", "ç¼©å†™", "ç®€å†™",
        "cp", "CP", "æƒ…ä¾£", "æ­æ¡£", "è§’è‰²", "ç™»åœºè§’è‰²", "ä¸»è¦è§’è‰²"
    ])

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Content-Type": "application/json"
    }

    try:
        # === 1. è·å– subject_id ===
        print("ğŸš€ è·å–æ¡ç›®ä¿¡æ¯...")
        search_url = "https://api.bgm.tv/v0/search/subjects"
        search_payload = {
            "keyword": anime_name,
            "filter": {"type": [2]},  # type=2 â†’ åŠ¨ç”»
            "sort": "rank",
            "limit": 3
        }
        res = requests.post(search_url, json=search_payload, headers=headers, timeout=10)
        res.raise_for_status()
        subject_list = res.json().get("data", [])

        if not subject_list:
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ¡ç›®")
            return list(keywords)

        subject = subject_list[0]
        subject_id = subject.get("id")
        name = subject.get("name", "")
        name_cn = subject.get("name_cn", "")
        print(f"ğŸ¯ subject_id: {subject_id} ï½œ åŸå: {name} ï½œ ä¸­æ–‡å: {name_cn}")

        keywords.update([name.strip(), name_cn.strip()])

        # === 2. æå– other_namesï¼ˆè‹±æ–‡åˆ«å/ç®€å†™ç­‰ï¼‰===
        for other in subject.get("other_names", []):
            if isinstance(other, str) and other.strip():
                keywords.add(other.strip())

        # === 3. æå– infobox ä¸­æ¡ç›®åˆ«åã€CPã€è‹±æ–‡åç­‰ ===
        detail_url = f"https://api.bgm.tv/v0/subjects/{subject_id}"
        detail_res = requests.get(detail_url, headers=headers, timeout=10)
        detail_res.raise_for_status()
        detail_data = detail_res.json()
        infobox = detail_data.get("infobox", [])

        has_role_info = False

        for entry in infobox:
            key = entry.get("key", "").strip().lower()
            val = entry.get("value", "")
            if any(valid.lower() in key for valid in valid_keys):
                if "è§’" in key:
                    has_role_info = True

                if isinstance(val, list):
                    for v in val:
                        v = v.get("v", "") if isinstance(v, dict) else v
                        if isinstance(v, str) and v.strip():
                            keywords.add(v.strip())
                elif isinstance(val, str):
                    for part in re.split(r"[ã€ï¼Œ,/\s]", val):
                        if part.strip():
                            keywords.add(part.strip())

        # === 4. è§’è‰²åˆ—è¡¨æå– name, name_cn + infobox ä¸­åˆ«å ===
        print("ğŸ“¥ è·å–è§’è‰²åˆ—è¡¨...")
        char_url = f"https://api.bgm.tv/v0/subjects/{subject_id}/characters"
        char_res = requests.get(char_url, headers=headers, timeout=10)
        characters = char_res.json()
        print(f"âœ… å…±æ‰¾åˆ° {len(characters)} ä¸ªè§’è‰²")

        for char in characters:
            char_id = char.get("id")
            name = char.get("name", "").strip()
            name_cn = char.get("name_cn", "").strip()
            if is_valid_character_name(name):
                keywords.add(name)
            if is_valid_character_name(name_cn):
                keywords.add(name_cn)

            # è·å–è§’è‰²è¯¦æƒ…
            detail_url = f"https://api.bgm.tv/v0/characters/{char_id}"
            detail_res = requests.get(detail_url, headers=headers, timeout=10)
            detail_data = detail_res.json()
            char_infobox = detail_data.get("infobox", [])

            for entry in char_infobox:
                key = entry.get("key", "").lower()
                val = entry.get("value", "")
                if any(k in key for k in ["åˆ«å", "ç®€ç§°", "cp", "ç§°å‘¼", "åˆå«", "å¤–å·"]):
                    if isinstance(val, list):
                        for item in val:
                            v = item.get("v", "") if isinstance(item, dict) else item
                            if v and len(v.strip()) >= 2 and is_valid_character_name(v):
                                keywords.add(v.strip())
                    elif isinstance(val, str):
                        for part in re.split(r"[ã€ï¼Œ,/\s]", val):
                            if part.strip() and is_valid_character_name(part):
                                keywords.add(part.strip())

    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼š{e}")

    # === 5. æ ‡å‡†åŒ–ã€å»é‡ã€å»ç©ºã€æ’é™¤é—²è§’ ===
    final_keywords = set()
    for k in keywords:
        k_norm = normalize_keyword(k)
        if len(k_norm) >= 2 and is_valid_character_name(k_norm):
            final_keywords.add(k_norm)

    final_list = sorted(final_keywords)
    print(f"ğŸ“Œ æœ€ç»ˆå…³é”®è¯ï¼ˆ{len(final_list)} é¡¹ï¼‰ï¼š{final_list}")
    return final_list


# === æ­¥éª¤ 3ï¼šæ„é€ æœç´¢å…³é”®è¯ï¼ˆè¡ç”Ÿè¯ Ã— äºŒåˆ›åç¼€ï¼‰ ===

def generate_search_keywords(extended_keywords):
    """ä»…ä½¿ç”¨æ‰©å±•å…³é”®è¯ï¼Œä¸æ·»åŠ åç¼€"""
    return extended_keywords


# === æ­¥éª¤ 4ï¼šB ç«™æœç´¢è§†é¢‘ ===
def bvid_search(keyword, page):
    url = "https://api.bilibili.com/x/web-interface/search/type"
    params = {
        "search_type": "video",
        "keyword": keyword,
        "page": page
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        "Referer": "https://www.bilibili.com/",
        "Origin": "https://www.bilibili.com",
        "Accept": "application/json",
        "Accept-Language": "zh-CN,zh;q=0.9",
        # âœ… å¤åˆ¶ä½ ç™»å½•çŠ¶æ€ä¸‹çš„ Cookie æ”¾åœ¨æ­¤å¤„
        "Cookie": BILIBILI_COOKIE
    }

    try:
        res = requests.get(url, params=params, headers=headers)
        res.raise_for_status()
        return res.json().get("data", {}).get("result", [])
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥ï¼š{keyword} - ç¬¬{page}é¡µ - é”™è¯¯ï¼š{e}")
        return []


# === æ­¥éª¤ 5ï¼šåŒ¹é…è§„åˆ™åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡ä½œå“äºŒåˆ›è§†é¢‘ ===
def contains_any(text, keywords):
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰"""
    return any(kw.lower() in text.lower() for kw in keywords)


# === æ­¥éª¤ 6ï¼šä¸»é€»è¾‘çˆ¬å– ===
def is_official_author(author_name):
    """åˆ¤æ–­UPä¸»æ˜¯å¦ä¸ºå®˜æ–¹è´¦å·"""
    OFFICIAL_USER_KEYWORDS = ["å®˜æ–¹", "å‡ºå“", "åˆ¶ä½œå§”å‘˜ä¼š", "åŠ¨ç”»å®˜æ–¹"]
    return any(keyword in author_name for keyword in OFFICIAL_USER_KEYWORDS)


def contains_official_tag(tags):
    """åˆ¤æ–­æ ‡ç­¾ä¸­æ˜¯å¦åŒ…å«å®˜æ–¹æ ‡ç­¾"""
    OFFICIAL_TAGS = ["PV", "å…ˆå¯¼", "é¢„å‘Š", "å®£ä¼ ", "ç‰‡æ®µ", "èŠ±çµ®", "å®˜æ–¹"]
    return contains_any(tags, OFFICIAL_TAGS)


def collect_fanmade_videos(anime_list):
    """æ”¶é›†æ‰€æœ‰åŠ¨æ¼«çš„äºŒåˆ›è§†é¢‘æ•°æ®ï¼ˆå¿…é¡»æ»¡è¶³ä¸‰å¤§ç­›é€‰æ¡ä»¶ï¼‰"""
    results = []

    for anime_name in tqdm(anime_list, desc="å¤„ç†åŠ¨æ¼«ä¸­"):
        extended_keywords = get_extended_keywords(anime_name)
        search_keywords = generate_search_keywords(extended_keywords)

        # åŠ¨æ¼«ä¸»åï¼ˆç”¨äº tag ä¸­â€œå®˜æ–¹åç§°â€åˆ¤æ–­ï¼‰
        main_names = [anime_name]
        main_names += [kw for kw in extended_keywords if kw != anime_name and len(kw) >= 2 and anime_name in kw]

        for keyword in search_keywords:
            for page in range(1, MAX_PAGES_PER_KEYWORD + 1):
                videos = bvid_search(keyword, page)
                time.sleep(0.5)

                for video in videos:
                    title = video.get("title", "")
                    desc = video.get("description", "")
                    tags = video.get("tag", "")  # é€—å·åˆ†éš”å­—ç¬¦ä¸²
                    author = video.get("author", "")  # UPä¸»åç§°

                    # === ä¸‰å¤§åˆ¤æ–­æ¡ä»¶ ===
                    # 1ï¸âƒ£ æ ‡é¢˜ä¸­åŒ…å«æ‰©å±•è¡ç”Ÿè¯
                    title_contains_keyword = contains_any(title, extended_keywords)

                    # 2ï¸âƒ£ æ ‡ç­¾ä¸­å¿…é¡»åŒæ—¶åŒ…å«ï¼šäºŒåˆ›æ ‡ç­¾ + å®˜æ–¹åç§°
                    tag_contains_fanmade = contains_any(tags, FANMADE_TAGS)
                    tag_contains_anime_name = contains_any(tags, main_names)

                    # 3ï¸âƒ£ éå®˜æ–¹å‘å¸ƒ
                    not_official = not contains_official_tag(tags) and not is_official_author(author)

                    # âœ… åŒæ—¶æ»¡è¶³ä¸‰æ¡æ ‡å‡†
                    if title_contains_keyword and tag_contains_fanmade and tag_contains_anime_name and not_official:
                        results.append({
                            "æ‰€å±åŠ¨æ¼«": anime_name,
                            "è§†é¢‘æ ‡é¢˜": title,
                            "æ’­æ”¾é‡": video.get("play"),
                            "å¼¹å¹•æ•°": video.get("video_review"),
                            "æ”¶è—æ•°": video.get("favorites"),
                            "UPä¸»": author,
                            "æ ‡ç­¾": tags,
                            "bvid": video.get("bvid"),
                            "url": f"https://www.bilibili.com/video/{video.get('bvid')}"
                        })

    return results

        # === æ­¥éª¤ 7ï¼šä¿å­˜ç»“æœ ===
if __name__ == "__main__":
    anime_names = read_anime_names(INPUT_CSV)  # è¯»å–åŠ¨æ¼«åˆ—è¡¨
    video_data = collect_fanmade_videos(anime_names)  # æ”¶é›†äºŒåˆ›è§†é¢‘

    # åˆ›å»ºDataFrameå¹¶ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(video_data)
    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")  # ä½¿ç”¨BOMå¤´ä¿å­˜ä¸­æ–‡

    print(f"\nâœ… å…±ä¿å­˜ {len(df)} æ¡è§†é¢‘è®°å½•åˆ° {OUTPUT_CSV}")