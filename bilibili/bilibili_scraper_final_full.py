# ===ã€0. å…¬å…±å¯¼å…¥å’Œé…ç½®ã€‘===
import os
import re
import time
import json
import requests
import unicodedata
import pandas as pd
from tqdm import tqdm
from datetime import datetime
from dotenv import load_dotenv
from functools import reduce
from hashlib import md5
import urllib.parse



load_dotenv()
BILIBILI_COOKIE = os.getenv("BILIBILI_COOKIE")

# ========== ğŸ§© 1. æ’å…¥ï¼šcatchinformationfinalver.py æ‰€æœ‰å‡½æ•°å®šä¹‰ï¼ˆä¸å« if __name__ == '__main__'ï¼‰ ==========
# === ç¯å¢ƒå˜é‡ä¸é…ç½®è¯»å– ===
load_dotenv()
BILIBILI_COOKIE = os.getenv("BILIBILI_COOKIE")

# === æ–‡ä»¶è·¯å¾„é…ç½® ===
INPUT_CSV = "C:/Code/python/animeproject/anime_list.csv"
OUTPUT_CSV = "C:/Code/python/animeproject/fanmade_results.csv"
MAX_PAGES_PER_KEYWORD = 3

# === äºŒåˆ›ç›¸å…³å…³é”®è¯å®šä¹‰ ===
FANMADE_SUFFIXES = [" MAD", " äºŒåˆ›", " æ··å‰ª", " åŒäºº", " AMV", " é¬¼ç•œ"]
FANMADE_TAGS = ["äºŒåˆ›", "MAD", "æ··å‰ª", "åŒäºº", "é¬¼ç•œ", "AMV", "å‰ªè¾‘"]

# === é—²è§’å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤æ— æ„ä¹‰è§’è‰²ï¼‰ ===
EXCLUDE_ROLE_KEYWORDS = [
    "ãƒ¢ãƒ–", "ãƒ¢ãƒ–ã‚­ãƒ£ãƒ©", "ãƒ¢ãƒ–ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼", "ã‚¨ã‚­ã‚¹ãƒˆãƒ©",
    "è·¯äºº", "ç¾¤ä¼—", "ç¾¤æ¼”", "å°å…µ", "å­¦ç”Ÿ", "è€å¸ˆ", "è®°è€…", "åŠ©æ‰‹",
    "é€šè¡Œäºº", "è§‚ä¼—", "æ‘äºº", "æ‘æ°‘", "ç”·æ€§", "å¥³æ€§", "å°‘å¹´", "å°‘å¥³",
    "äººç±»", "åº—å‘˜", "ä¿å®‰", "å¸æœº", "åŒ»ç”Ÿ", "å£«å…µ", "ä¹˜å®¢", "æœåŠ¡å‘˜",
    "extra", "mob character", "é—²è§’"
]

# === å·¥å…·å‡½æ•° ===
def normalize_keyword(text):
    """æ ‡å‡†åŒ–å…³é”®è¯ï¼ˆå…¨è§’è½¬åŠè§’å¹¶å»é™¤ç©ºæ ¼ï¼‰"""
    return unicodedata.normalize('NFKC', text).strip() if isinstance(text, str) else ""

def is_valid_character_name(name):
    """åˆ¤æ–­åå­—æ˜¯å¦æœ‰æ•ˆä¸”ä¸å±äºé—²è§’"""
    return bool(name) and not any(x in name for x in EXCLUDE_ROLE_KEYWORDS)

def contains_any(text, keywords):
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯"""
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in keywords)

def read_anime_names(csv_path):
    """ä»CSVæ–‡ä»¶ä¸­è¯»å–åŠ¨æ¼«åç§°åˆ—è¡¨"""
    df = pd.read_csv(csv_path)
    anime_list = df["anime"].dropna().tolist()
    print(f"âœ… æˆåŠŸè¯»å– {len(anime_list)} ä¸ªåŠ¨æ¼«åç§°")
    return anime_list

def generate_search_keywords(extended_keywords):
    """è¿”å›æ‰©å±•å…³é”®è¯åˆ—è¡¨"""
    return extended_keywords

# === Bangumi APIï¼šè·å–è¡ç”Ÿå…³é”®è¯ ===
def get_extended_keywords(anime_name):
    print(f"\nğŸ” æ­£åœ¨å¤„ç†ï¼š{anime_name}")
    keywords = {anime_name}
    headers = {"User-Agent": "Mozilla/5.0", "Content-Type": "application/json"}
    valid_keys = {"åˆ«å", "åˆ«ç§°", "åˆç§°", "è‹±æ–‡", "ç®€ç§°", "ç¼©å†™", "ç®€å†™", "cp", "æƒ…ä¾£", "è§’è‰²", "ç™»åœºè§’è‰²", "ä¸»è¦è§’è‰²"}

    try:
        # === 1. è·å– subject_id ===
        search_url = "https://api.bgm.tv/v0/search/subjects"
        res = requests.post(search_url, json={"keyword": anime_name, "filter": {"type": [2]}, "sort": "rank", "limit": 3}, headers=headers, timeout=10)
        subject_list = res.json().get("data", [])
        if not subject_list:
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ¡ç›®")
            return list(keywords)

        subject = subject_list[0]
        subject_id = subject.get("id")
        keywords.update(filter(None, [subject.get("name", "").strip(), subject.get("name_cn", "").strip()]))
        keywords.update(subject.get("other_names", []))

        # === 2. è·å– infobox ä¿¡æ¯ ===
        detail_url = f"https://api.bgm.tv/v0/subjects/{subject_id}"
        infobox = requests.get(detail_url, headers=headers, timeout=10).json().get("infobox", [])

        for entry in infobox:
            key = entry.get("key", "").lower()
            val = entry.get("value", "")
            if any(k.lower() in key for k in valid_keys):
                items = [v.get("v", "") if isinstance(v, dict) else v for v in val] if isinstance(val, list) else re.split(r"[ã€ï¼Œ,\s/]", val)
                keywords.update(filter(None, map(str.strip, items)))

        # === 3. è·å–è§’è‰²åŠåˆ«å ===
        char_url = f"https://api.bgm.tv/v0/subjects/{subject_id}/characters"
        characters = requests.get(char_url, headers=headers, timeout=10).json()
        print(f"âœ… å…±æ‰¾åˆ° {len(characters)} ä¸ªè§’è‰²")

        for char in characters:
            for name in [char.get("name", ""), char.get("name_cn", "")]:
                name = name.strip()
                if is_valid_character_name(name):
                    keywords.add(name)

            # è·å–è§’è‰²åˆ«å
            detail_data = requests.get(f"https://api.bgm.tv/v0/characters/{char['id']}", headers=headers, timeout=10).json()
            for entry in detail_data.get("infobox", []):
                if any(k in entry.get("key", "").lower() for k in ["åˆ«å", "ç®€ç§°", "cp", "ç§°å‘¼", "åˆå«", "å¤–å·"]):
                    val = entry.get("value", "")
                    items = [v.get("v", "") if isinstance(v, dict) else v for v in val] if isinstance(val, list) else re.split(r"[ã€ï¼Œ,\s/]", val)
                    keywords.update(filter(lambda x: is_valid_character_name(x.strip()), map(str.strip, items)))

    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼š{e}")

    # === æ ‡å‡†åŒ–ã€å»é‡ã€è¿‡æ»¤ ===
    final_keywords = {normalize_keyword(k) for k in keywords if len(normalize_keyword(k)) >= 2 and is_valid_character_name(normalize_keyword(k))}
    final_list = sorted(final_keywords)
    print(f"ğŸ“Œ æœ€ç»ˆå…³é”®è¯ï¼ˆ{len(final_list)} é¡¹ï¼‰ï¼š{final_list}")
    return final_list

# === B ç«™æœç´¢æ¥å£ ===
def bvid_search(keyword, page):
    url = "https://api.bilibili.com/x/web-interface/search/type"
    params = {"search_type": "video", "keyword": keyword, "page": page}
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.bilibili.com/",
        "Origin": "https://www.bilibili.com",
        "Accept": "application/json",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Cookie": BILIBILI_COOKIE
    }
    try:
        res = requests.get(url, params=params, headers=headers)
        return res.json().get("data", {}).get("result", [])
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥ï¼š{keyword} - ç¬¬{page}é¡µ - é”™è¯¯ï¼š{e}")
        return []

# === äºŒåˆ›è§†é¢‘ç­›é€‰é€»è¾‘ ===
def is_official_author(name):
    return any(k in name for k in ["å®˜æ–¹", "å‡ºå“", "åˆ¶ä½œå§”å‘˜ä¼š", "åŠ¨ç”»å®˜æ–¹"])

def contains_official_tag(tags):
    return contains_any(tags, ["PV", "å…ˆå¯¼", "é¢„å‘Š", "å®£ä¼ ", "ç‰‡æ®µ", "èŠ±çµ®", "å®˜æ–¹"])

def collect_fanmade_videos(anime_list):
    results = []
    for anime_name in tqdm(anime_list, desc="å¤„ç†åŠ¨æ¼«ä¸­"):
        extended_keywords = get_extended_keywords(anime_name)
        search_keywords = generate_search_keywords(extended_keywords)
        main_names = [anime_name] + [kw for kw in extended_keywords if anime_name in kw and kw != anime_name]

        for keyword in search_keywords:
            for page in range(1, MAX_PAGES_PER_KEYWORD + 1):
                for video in bvid_search(keyword, page):
                    title = video.get("title", "")
                    desc = video.get("description", "")
                    tags = video.get("tag", "")
                    author = video.get("author", "")

                    if (
                        contains_any(title, extended_keywords)
                        and contains_any(tags, FANMADE_TAGS)
                        and contains_any(tags, main_names)
                        and not contains_official_tag(tags)
                        and not is_official_author(author)
                    ):
                        results.append({
                            "æ‰€å±åŠ¨æ¼«": anime_name,
                            "è§†é¢‘æ ‡é¢˜": title,
                            "æ’­æ”¾é‡": video.get("play"),
                            "å¼¹å¹•æ•°": video.get("video_review"),
                            "æ”¶è—æ•°": video.get("favorites"),
                            "UPä¸»": author,
                            "æ ‡ç­¾": tags,
                            "bvid": video.get("bvid"),
                            "url": f"https://www.bilibili.com/video/{video.get('bvid')}"
                        })
                time.sleep(0.5)
    return results

# ========== ğŸ§© 2. æ’å…¥ï¼šcomment_finder_2(2).py æ‰€æœ‰å‡½æ•°å®šä¹‰ ==========


SEARCH_API_URL = "https://api.bilibili.com/x/web-interface/search/type"
EPISODE_LIST_API_URL = "https://api.bilibili.com/pgc/view/web/season"
COMMENT_API_URL = "https://api.bilibili.com/x/v2/reply/wbi/main"

HEADERS = {
    'Accept': '*/*',
    'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
    'Connection': 'keep-alive',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
    'Referer': 'https://www.bilibili.com/',
    "Cookie": os.getenv("BILIBILI_COOKIE")
}

# --- 2. WBIç­¾å ---
mixinKeyEncTab = [
    46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,
    33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,
    61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,
    36, 20, 34, 44, 52
]


def getMixinKey(orig: str):
    return reduce(lambda s, i: s + orig[i], mixinKeyEncTab, '')[:32]


def encWbi(params: dict, img_key: str, sub_key: str):
    mixin_key = getMixinKey(img_key + sub_key)
    curr_time = round(time.time())
    params['wts'] = curr_time
    params = dict(sorted(params.items()))
    params = {
        k: ''.join(filter(lambda chr: chr not in "'!()*", str(v)))
        for k, v in params.items()
    }
    query = urllib.parse.urlencode(params)
    wbi_sign = md5((query + mixin_key).encode()).hexdigest()
    params['w_rid'] = wbi_sign
    return params


'è·å–æœ€æ–°çš„ img_key å’Œ sub_key,æ³¨æ„è¯¥å¯†é’¥æ¯æ—¥æ›´æ–°'


def getWbiKeys() -> tuple[str, str] | None:
    try:
        resp = requests.get('https://api.bilibili.com/x/web-interface/nav', headers=HEADERS)
        resp.raise_for_status()
        json_content = resp.json()
        img_url: str = json_content['data']['wbi_img']['img_url']
        sub_url: str = json_content['data']['wbi_img']['sub_url']
        img_key = img_url.rsplit('/', 1)[1].split('.')[0]
        sub_key = sub_url.rsplit('/', 1)[1].split('.')[0]
        return img_key, sub_key
    except Exception as e:
        print(f"[!] è·å–WBIå¯†é’¥å¤±è´¥: {e}")
        return None


# --- 3. æ•°æ®è·å– ---
'å›½åˆ›åç§°-> season_id'


def find_bangumi_season_id(keyword: str) -> dict | None:
    print(f"[*] æ­¥éª¤ 1: æ­£åœ¨ä¸ºå…³é”®è¯ '{keyword}' æœç´¢ç•ªå‰§...")
    params = {'search_type': 'media_bangumi', 'keyword': keyword}
    try:
        response = requests.get(SEARCH_API_URL, params=params, headers=HEADERS, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get('code') == 0:
            results = data.get('data', {}).get('result')
            if not results: return None
            top_result = results[0]
            season_id = top_result.get('season_id')
            title = top_result.get('title', '').replace('<em class="keyword">', '').replace('</em>', '')
            if season_id:
                print(f"[âœ“] æˆåŠŸåŒ¹é…åˆ°ç•ªå‰§: '{title}', Season ID: {season_id}")
                return {'season_id': season_id, 'title': title}
    except Exception as e:
        print(f"[!] æ­¥éª¤1æ‰§è¡Œå¤±è´¥: {e}")
    return None


'season_id -> ep_id'


def get_all_episodes_info(season_id: int) -> list | None:
    print(f"\n[*] æ­¥éª¤ 2: æ­£åœ¨ç”¨ Season ID: {season_id} è·å–æ‰€æœ‰å‰§é›†çš„ä¿¡æ¯...")
    params = {'season_id': season_id}
    try:
        response = requests.get(EPISODE_LIST_API_URL, params=params, headers=HEADERS, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get('code') == 0:
            episodes = data.get('result', {}).get('episodes')
            if not episodes: return None
            episode_info_list = []
            for ep in episodes:
                if ep.get('aid'):
                    episode_info_list.append({'title': ep.get('long_title'), 'aid': ep.get('aid')})
            print(f"[âœ“] æˆåŠŸè·å–åˆ° {len(episode_info_list)} é›†çš„æœ‰æ•ˆä¿¡æ¯.")
            return episode_info_list
    except Exception as e:
        print(f"[!] æ­¥éª¤2æ‰§è¡Œå¤±è´¥: {e}")
    return None


def fetch_comments_with_wbi(aid: int, episode_title: str, wbi_keys: tuple[str, str], scrape_all_pages: bool):
    """
    æ­¥éª¤3: æŠ“å–è¯„è®ºã€‚
    """
    all_comments_text = []
    total_comment_count = 0
    pagination_str = json.dumps({"offset": ""})
    print(f"  -> å¼€å§‹å¤„ç† '{episode_title}' (aid: {aid}) çš„è¯„è®ºæŠ“å–...")

    while True:
        params = {'oid': aid, 'type': 1, 'mode': 3, 'pagination_str': pagination_str, 'plat': 1}
        signed_params = encWbi(params.copy(), img_key=wbi_keys[0], sub_key=wbi_keys[1])
        print(f"    - æ­£åœ¨è¯·æ±‚ (cursor: {params['pagination_str'][:40]}...)...")
        try:
            response = requests.get(COMMENT_API_URL, params=signed_params, headers=HEADERS, timeout=20)
            response.raise_for_status()
            data = response.json()
            if data.get('code') == 0:
                if data.get('data', {}).get('cursor', {}).get('all_count') is not None:
                    total_comment_count = data['data']['cursor']['all_count']

                replies = data.get('data', {}).get('replies')
                if replies:
                    for reply in replies:
                        all_comments_text.append(reply.get('content', {}).get('message'))
                    print(f"    [âœ“] æœ¬é¡µå¤„ç†å®Œæ¯•ï¼Œè·å¾— {len(replies)} æ¡è¯„è®ºã€‚")

                if not scrape_all_pages:
                    print(f"    [i] å·²è®¾ç½®ä¸ºåªæŠ“å–ç¬¬ä¸€é¡µï¼Œåœæ­¢ç¿»é¡µã€‚")
                    break

                if data.get('data', {}).get('cursor', {}).get('is_end'):
                    print(f"    [i] å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢ç¿»é¡µã€‚")
                    break

                pagination_str = data['data']['cursor']['pagination_reply']['next_offset']
                time.sleep(2)
            else:
                print(f"    [!] APIè¿”å›é”™è¯¯: {data.get('message')}, åœæ­¢ç¿»é¡µã€‚")
                break
        except Exception as e:
            print(f"    [!] è¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}, åœæ­¢æŠ“å–æœ¬é›†ã€‚")
            break

    print(
        f"  [âœ“] '{episode_title}' è¯„è®ºæŠ“å–å®Œæˆã€‚å…± {len(all_comments_text)} æ¡(å·²åŠ è½½) / {total_comment_count} æ¡(æ€»è®¡)ã€‚")
    return total_comment_count, all_comments_text


def get_season_follower_count(season_id: int) -> int:
    """
    æ ¹æ®ç»™å®šçš„season_idï¼Œä»ç•ªå‰§è¯¦æƒ…APIä¸­è·å–å‡†ç¡®çš„è¿½ç•ªäººæ•°(series_follow)ã€‚

    :param season_id: ç•ªå‰§çš„Season IDã€‚
    :return: è¿½ç•ªäººæ•° (int)ã€‚å¦‚æœè·å–å¤±è´¥ï¼Œåˆ™è¿”å› -1ã€‚
    """
    params = {'season_id': season_id}

    try:
        response = requests.get(EPISODE_LIST_API_URL, params=params, headers=HEADERS, timeout=10)
        response.raise_for_status()
        data = response.json()

        if data.get('code') == 0:

            seasons_list = data.get('result', {}).get('seasons')
            if seasons_list:

                for season_item in seasons_list:
                    if season_item.get('season_id') == season_id:

                        stat_data = season_item.get('stat')
                        if stat_data:

                            followers = stat_data.get('series_follow')
                            if followers is not None:
                                print(f"[âœ“] æˆåŠŸè·å–è¿½ç•ªäººæ•° (series_follow): {followers}")
                                return followers
                print(f"[!] åœ¨seasonsåˆ—è¡¨ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„ season_id: {season_id}ã€‚")
                return -1
            else:
                print("[!] APIå“åº”æˆåŠŸï¼Œä½†æœªæ‰¾åˆ° 'seasons' åˆ—è¡¨ã€‚")
                return -1
        else:
            print(f"[!] APIè¿”å›é”™è¯¯: {data.get('message')}")
            return -1

    except requests.exceptions.RequestException as e:
        print(f"[!] è¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return -1
    except Exception as e:
        print(f"[!] ç¨‹åºå‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return -1




# ========== ğŸ§© 3. æ’å…¥ï¼šguochuang_hotlist(1).py çš„ fetch_category_ranking å‡½æ•° ==========

# --- 1. urlé…ç½® ---

RANKING_API_URL = "https://api.bilibili.com/x/web-interface/ranking/v2"

# åªéœ€è¦ä¸€ä¸ªç®€å•çš„User-Agentå³å¯
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
}


# --- 2. æ ¸å¿ƒæŠ“å–å‡½æ•° ---
def fetch_category_ranking(rid: int) -> list | None:
    """
    æ ¹æ®ç»™å®šçš„åˆ†åŒºID(rid)è·å–Bç«™è§†é¢‘æ’è¡Œæ¦œã€‚

    :param rid: ç›®æ ‡åˆ†åŒºçš„IDã€‚
    :return: åŒ…å«æ¦œå•æ‰€æœ‰è§†é¢‘ä¿¡æ¯çš„åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›Noneã€‚
    """
    params = {
        'rid': rid,
        'type': 'all'  # allè¡¨ç¤ºå…¨éƒ¨åˆ†ç¨¿ä»¶ç±»å‹
    }

    # æ ¹æ®ridç”Ÿæˆä¸€ä¸ªå¯è¯»çš„åˆ†åŒºåç”¨äºæ˜¾ç¤º
    category_name = f"åˆ†åŒº(rid={rid})"
    if rid == 0:
        category_name = "å…¨ç«™"

    print(f"[*] æ­£åœ¨å‘Bç«™APIè¯·æ±‚ [{category_name}] çš„æ’è¡Œæ¦œæ•°æ®...")

    try:
        response = requests.get(RANKING_API_URL, params=params, headers=HEADERS, timeout=15)
        response.raise_for_status()
        data = response.json()

        if data.get('code') == 0:
            chart_list = data.get('data', {}).get('list')
            if chart_list:
                print(f"æˆåŠŸè·å–åˆ° {len(chart_list)} æ¡æ¦œå•æ•°æ®.")
                return chart_list
            else:
                print("[!] APIè¿”å›æˆåŠŸï¼Œä½†æ¦œå•åˆ—è¡¨ä¸ºç©ºã€‚")
                return None
        else:
            print(f"[!] APIè¿”å›é”™è¯¯: {data.get('message')}")
            return None

    except Exception as e:
        print(f"[!] è¯·æ±‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None




# ========== ğŸš€ 4. ä¸»äº¤äº’å¼èœå•å…¥å£ ==========
def main():
    while True:
        print("\nğŸ“Œ è¯·é€‰æ‹©è¦æ‰§è¡Œçš„åŠŸèƒ½ï¼š")
        print("1. æŠ“å– Bç«™å›½åˆ›æ’è¡Œæ¦œ")
        print("2. æŠ“å–æŸéƒ¨ç•ªå‰§çš„æ‰€æœ‰å‰§é›†è¯„è®º")
        print("3. æŠ“å–å¤šä¸ªåŠ¨æ¼«çš„äºŒåˆ›è§†é¢‘")
        print("0. é€€å‡ºç¨‹åº")

        choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·: ").strip()

        if choice == "1":
            try:
                rid = int(input("è¯·è¾“å…¥åˆ†åŒº ridï¼ˆå¦‚ï¼š168=å›½åˆ›ï¼‰: "))
                data = fetch_category_ranking(rid)
                if data:
                    result_list = []
                    for rank, item in enumerate(data, 1):
                        stat = item.get("stat", {})
                        result_list.append({
                            'æ’å': rank,
                            'æ ‡é¢˜': item.get("title"),
                            'BVID': item.get("bvid"),
                            'UPä¸»': item.get("owner", {}).get("name"),
                            'æ’­æ”¾é‡': stat.get("view", 0),
                            'å¼¹å¹•æ•°': stat.get("danmaku", 0),
                            'è¯„è®ºæ•°': stat.get("reply", 0),
                            'ç‚¹èµæ•°': stat.get("like", 0),
                            'æŠ•å¸æ•°': stat.get("coin", 0),
                            'æ”¶è—æ•°': stat.get("favorite", 0),
                            'åˆ†äº«æ•°': stat.get("share", 0),
                            'è§†é¢‘é“¾æ¥': f"https://www.bilibili.com/video/{item.get('bvid')}"
                        })
                    df = pd.DataFrame(result_list)
                    date_str = datetime.now().strftime("%Y-%m-%d")
                    filename = f"bilibili_ranking_rid_{rid}_{date_str}.csv"
                    df.to_csv(filename, index=False, encoding="utf-8-sig")
                    print(f"âœ… å·²ä¿å­˜æ’è¡Œæ¦œæ•°æ®åˆ° {filename}")
            except Exception as e:
                print(f"[!] é”™è¯¯: {e}")

        elif choice == "2":
            work_name = input("è¯·è¾“å…¥ç•ªå‰§åç§°: ").strip()
            if not work_name:
                print("è¾“å…¥ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue
            scrape_all = input("æ˜¯å¦æŠ“å–å…¨éƒ¨è¯„è®ºï¼Ÿ1=å¦ï¼ˆé»˜è®¤ï¼‰/ 2=æ˜¯: ").strip() == "2"
            wbi_keys = getWbiKeys()
            if not wbi_keys:
                continue
            bangumi_info = find_bangumi_season_id(work_name)
            if not bangumi_info:
                continue
            episodes = get_all_episodes_info(bangumi_info['season_id'])
            follower_count = get_season_follower_count(bangumi_info['season_id'])
            print(f"ğŸ¯ è¿½ç•ªäººæ•°: {follower_count}")
            if episodes:
                all_data = []
                for ep in episodes:
                    count, comments = fetch_comments_with_wbi(ep['aid'], ep['title'], wbi_keys, scrape_all)
                    all_data.append({
                        'episode_title': ep['title'],
                        'aid': ep['aid'],
                        'total_comment_count': count,
                        'comment_texts': comments
                    })
                    time.sleep(2)
                filename = f"bilibili_Comments_{bangumi_info['title']}.json"
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(all_data, f, ensure_ascii=False, indent=4)
                    print(f"âœ… è¯„è®ºæ•°æ®ä¿å­˜è‡³ {filename}")

        elif choice == "3":
            input_path = input("è¯·è¾“å…¥åŒ…å«åŠ¨æ¼«åç§°çš„CSVè·¯å¾„ï¼ˆå¦‚ anime_list.csvï¼‰: ").strip()
            output_path = input("è¯·è¾“å…¥è¾“å‡ºç»“æœCSVè·¯å¾„ï¼ˆå¦‚ fanmade_results.csvï¼‰: ").strip()
            anime_list = read_anime_names(input_path)
            results = collect_fanmade_videos(anime_list)
            pd.DataFrame(results).to_csv(output_path, index=False, encoding="utf-8-sig")
            print(f"âœ… å·²ä¿å­˜äºŒåˆ›è§†é¢‘è®°å½•åˆ° {output_path}")

        elif choice == "0":
            print("é€€å‡ºç¨‹åºã€‚")
            break
        else:
            print("æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡è¯•ã€‚")

# ========== ğŸ”š ç¨‹åºå…¥å£ç‚¹ ==========
if __name__ == "__main__":
    main()










